
\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{verbose,a4paper,tmargin=15mm,bmargin=30mm,lmargin=30mm,rmargin=20mm}
%\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{lipsum}
\usepackage{nomencl}
\makenomenclature
\usepackage{bibentry}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{gensymb}
\usepackage{refstyle}
\usepackage{setspace}
\usepackage{notoccite} %for sorting references
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{subcaption}
\usepackage{algpseudocode}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]
\setcounter{secnumdepth}{5}
\setcounter{tocdepth}{5}
\graphicspath{ {images/} }
\onehalfspacing
\def\compressible{\textit{compressible}\hspace{0.1in}}
\def\sparse{\textit{sparse}\hspace{0.1in}}
\def\ksparse{\textit{k-sparse}\hspace{0.1in}}
\def\measurement{\textit{measurement}\hspace{0.1in}}
\def\x{$x$\hspace{0.1in}}
\def\y{$y$\hspace{0.1in}}
%\def\cosamp{\textbf{CoSaMP}\hspace{0.1in}}
\def\cosamp{CoSaMP\hspace{0.1in}}

\newcounter{eqn}
\renewcommand*{\theeqn}{\alph{eqn})}
\newcommand{\num}{\refstepcounter{eqn}\text{\theeqn}\;}
\makeatletter
%for figure grid
\newcommand{\putindeepbox}[2][0.7\baselineskip]{{%
    \setbox0=\hbox{#2}%
    \setbox0=\vbox{\noindent\hsize=\wd0\unhbox0}
    \@tempdima=\dp0
    \advance\@tempdima by \ht0
    \advance\@tempdima by -#1\relax
    \dp0=\@tempdima
    \ht0=#1\relax
    \box0
}}
\makeatother
\linespread{2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\newcommand{\LyX}{L\kern-.1667em\lower.25em\hbox{Y}\kern-.125emX\spacefactor1000}

\makeatother

\begin{document}
\begin{titlepage}
\thispagestyle{empty}
\vspace*{0.7cm}
{\centering     
\large
{\Large\bf Compressed Sensing in Internet of Things}\\
\vspace{3cm}
\bf{Seminar Report}\\
\vspace{0.25cm}
\vspace{0.1cm}
\it
by \\
\vspace{.5cm}
\rm
{\large \bf {Nagma Samreen Khan}}\\
{\large \bf {153079030}}

\vspace{1cm}

{\it{under the guidance of}} \\
\vspace{.5cm}

\hspace{.05cm} {\large \bf {Prof. Kumar Appaiah}}\\
%\hspace{.05cm}
\vspace {0.5cm}
%\iitbseal\ \\

\begin{figure}[h] 
%\hspace{6cm}
%\vspace{5cm}
{\centering {\includegraphics[width=0.32\textwidth]{images/IITB_logo}}\par}
\end{figure} 

Department of Electrical Engineering \\ 
Indian Institute of Technology, Bombay\\ 
{\centering
\hspace{6.5cm}April 2016} 
}
\pagebreak 
\end{titlepage}

\begin{abstract}
Internet of Things is the emerging technology of this era and involves quite a lot of data which is being 
sensed and transmitted over the internet.
In most cases the sensed data is sparse, so we need efficient sampling techniques which exploits this property
and samples and transmits much less data i.e. if $x \in \mathbb{R}^n$ is
the quantity being originally sampled then sample $y \in \mathbb{R}^m$ where $m << n$. 
Also efficient reconstruction
algorithms are needed to reconstruct the original quantity with small error at the receiver's end.
\end{abstract}
\section{Introduction}
  \subsection{Internet of Things}
  Internet of Things or commonly referred to as IoT defines a paradigm in which `everyday' devices can be connected 
  together in a network and thus can communicate with each other over the internet. Moreover the connected devices are equipped 
  with `identifying', `sensing' and `processing' \cite{Whitmore-survey}
  capabilites which enable them to 
  achieve some objective or complete some assigned task. Nowadays IoT is widely used in applications like
  healthcare, utilities, transport, etc. The revolution in the field of IoT has enabled
  these devices to have interaction capabilites too i.e. they can be controlled, actuated and commanded.
  \cite{Gubbi-vision}. 
  \par The components of IoT broadly are Hardware, Software and Architecture. Hardware refers to the component/s
  or the device/s which constitue the system which has been designed to achieve some objective, Software refers roughly
  to the support which enables the devices in the system to exchange and process data inspite of their heterogeniety 
   %\cite{Whitmore-survey}
   and Architecture provides the specifications so that information transfer and processing
   as well as networking of devices can be done in a standard fashion.
   \par IoT has made its mark in healthcare by providing remote monitoring of patients health, in industries 
   by providing efficient tracking of goods, in `smart homes' by monitoring and controlling resource consumption.
   And it holds the promise of `smart cities' where pollution level to traffic will be monitored and controlled
   \cite{Whitmore-survey} \cite{Gubbi-vision}.
%   \par In healthcare applications, IoT can be used to monitor a patient's health condtion over the internet, and 
%   in case of critical patients the sensors placed on the patient can transmit and make information available continuously
%   to the doctors and patient's relatives which will lead to better monitoring and enable doctors to take timely action
%   in case of emergencies. IoT has already made its mark in making `smart homes' where sensors and actuators placed in 
%   the house or building complex continuously monitor and control the resource consumption and also fulfil security needs.
%   On a broader scale, IoT can be deployed to make `smart cities' in which everything starting from traffic to the levels 
%   of pollution can be monitored and controlled \cite{Whitmore-survey}. 
%   \par Even in the industries IoT can be used for quality management, improving efficiency and tracking of goods. 
%   In can be used in hazardous industries, say the mining industry, to improve the safety level
%   by monitoring the condition inside the mine via sensors and continuously process the data to provide better disaster
%   management and in-time warning systems. In transportation and logistics, the monitoring of movement of goods, say 
%   using Radio Frequency Identification( RFID ) can be 
%   made `real-time' using IoT and thus providing better tracking \cite{Gubbi-vision}.
%   \par These are just some examples of how IoT has affeced, is affecting and will affect our lives but one thing is clear
%   that in the coming years IoT will lead to further betterment  in our lives through saving our time, money and resources.
  \subsection{Compressed Sensing}
  With increase in the number of connected devices around us, the amount of data transmitted over the internet will naturally
  increase. Also with increase in the number of sensors, large amounts of data will be generated which will have to be 
  processed, stored and transmitted over the internet in a timely fashion. So consumption of resources by the entire system
  will increase. This is where Compressed or Compressive Sensing comes in. 
  \par The simplest way to define Compressed Sensing is that we measure and transmit much less information
  compared to what would have been actually measured by the sensors 
  %i.e. say \x and also \x
  and that can also be reconstructed with a 
  small error at the receiver's end.
  This opens up a world of possibilities as it leads to saving time and resources and thus data transmission can be done
  much faster. The only prerequisite is that the data has to \textit{sparse} or \textit{compressible}
  in some transform domain. These concepts
  are defined mathematically in the next section.
%   but here an example is presented to give a flavour of Compressed Sensing.
%   Say we want to measure temperature at every point in a large room and for that purpose say we deploy 100 sensors distributed
%   uniformly across the room. Now we know that temperature is a slowly varying quantity and thus will have mainly low
%   frequency components. So if we take the Fourier Transform of the data then we can ignore the higher frequency coefficients
%   and thus the measurements are compressible in Fourier Transform domain.
%  
\section{Mathematical Background to Compressed Sensing}
% \begin{defn}
%  A vector \textit{x} $\in$ $\mathbb{R}^n$ is said to be \textit{k-sparse} if there are \textit{k} 
%  non-zero coefficients in \textit{x}.
% \end{defn}
% 
% \begin{defn}
%  A vector \textit{x} $\in$ $\mathbb{R}^n$ is said to be \textit{compressible} if it is well-approximated by its
%  \textit{k}-term \sparse approximation \textit{z} i.e. $\sigma_k$ is small where \cite{CS_book},
%  \begin{equation}
% %  \sigma_k = \inf_{\norm{z_0}_0 \leq k}
% \sigma_k = \inf_{\|z_0\|_0 \leq k} \| x - z \|_p
% \end{equation}
% \end{defn}
 A vector \textit{x} $\in$ $\mathbb{R}^n$ is said to be \textit{k-sparse} if there are \textit{k} 
 non-zero coefficients in \textit{x}. 
 And the vector \textit{x} $\in$ $\mathbb{R}^n$ is said to be \textit{compressible} if it is well-approximated by its
\textit{k}-term \sparse approximation \textit{z} and approximation error is small. Also
\textit{support} of a vector \x i.e. $supp(x)$ is defined as the set of indices for which the value of \x is
non-zero \cite{CS_book}.
% \begin{defn}
%  Support of a sparse vector \x i.e. $supp(x)$ is defined as,
%  \begin{equation}
%   supp(x) = \{i | x(i) \neq 0 \}
%  \end{equation}
%  Note that for \ksparse signals $\| supp(x) \|_0 \leq k$.
% \end{defn}

In this discussion we are assuming that \textit{x} is \sparse or \compressible in its original domain and no transform
is needed. We can make this assumption without loss of generality because any signal in $\mathbb{R}^n$ can be 
represented in terms on a basis matrix (assumed orthonormal) \cite{Baraniuk-CS}.
%, say $\psi$ where $\psi_i$ vectors are its columns, so \x can be written as,
% \begin{equation}
%  x = \sum_{i=1}^{n} s_i \psi_i = \textbf{s} \psi 
% \end{equation}
% where \textit{s} $\in \mathbb{R}^n$. So \x and \textit{s} are considered equivalent \cite{Baraniuk-CS}. 
Also whenever a vector is mentioned as \ksparse then either it actually is or it is \compressible and can be well approximated 
by its \ksparse representation.

\par Now suppose \textit{x}$\in$ $\mathbb{R}^n$ is the original data measured by the sensors and 
is known to be \ksparse then we don't measure \x directly but instead measure \y$\in$ $\mathbb{R}^m$, $m << n$ where,
\begin{equation}
 y = \phi x
\end{equation}
where $\phi$ is $m\times n$ matrix, called the \measurement matrix, and is required to satisfy the \textit{restricted
isometry property}( RIP) \cite{Needell-CoSaMP}
%( see Def. ~\ref{def:RIP} ) 
and also it is required to be \textit{incoherent} with the basis matrix $\psi$ 
i.e. the rows of $\phi$ cannot sparsely represent the columns of $\psi$ \cite{Baraniuk-CS}.
% \begin{defn}
%  For $\phi$ to satisfy $2k$\textit{-restricted isometry property} ($2k$-RIP), the property states that 
%  for any two vectors $x_1$ and $x_2$ and for
%  a $\delta_{2k} \in (0,1)$, the following must hold,
%   \begin{equation}
%   (1 - \delta_{2k}) \leq \frac{\|\phi x_1-\phi x_2 \|_2}{\|x_1-x_2\|_2} \leq (1 + \delta_{2k}) \hspace{0.1in}
%   where \hspace{0.1in} \|x_1\|_0, \|x_2\|_0 \leq k.
%  \end{equation}
%  or equivalently,
%  \begin{equation}
%   (1 - \delta_{2k}) \leq \frac{\|\phi x'\|_2}{\|\ x'\|_2} \leq (1 + \delta_{2k}) \hspace{0.1in}
%   where \hspace{0.1in} x'=|x_1-x_2|.
%  \end{equation}
%  The smallest $\delta_{2k}$ for which the $2k$-RIP is satisfied is called the 
%  $2k$th RIP constant of the matrix $\phi$ .
%  \label{def:RIP}
%   \end{defn}
%   It is evident that $x'$ will be $2k$-sparse as it is difference of two \ksparse signals.
  Satisfying the RIP property simply means that the distance (here $l_2$ norm) between two \ksparse vectors is preserved
  during transformation with matrix $\phi$.
  
 \par It has been shown that a $m \times n$ matrix $\phi$ having iid entries from a Gaussian distribution
  possesess \textit{RIP} with high probability if $m \geq c\hspace{0.1in}log(\frac{n}{k})$, 
 where \textit{c} is small constant \cite{Baraniuk-CS}.

\section{Recovery Algorithms}
Given a measurement vector \y $\in \mathbb{R}^m$, which may be noisy, reconstructing the original \ksparse vector \x $\in \mathbb{R}^n$
requires searching over all $\binom{n}{k}$ subspaces. The challenge for the recovery algorithms is to identify 
which $k$ columns of $\phi$ span the subspace in which \y lies \cite{Dai-subspace}.
%As $m << n$, $\phi$ is spanning only \textit{m} dimensions,
%and the \textit{n-m} dimensions lie in its nullspace. So, there can be many vectors of the form $x'=x+r$, where $r$ belongs
%to null space of $\phi$, satisfying $\phi x' = y$. 
\subsection{CoSaMP}
% In this discussion, we will be focussing on the \textit{Greedy Pursuit} algorithms, that tries to solve the problem
% by achieving the best performance for the particular instant with the hope that it will give an overall best performance.
% On of the popular Greedy Pursuit Recovery algorithm for Compressed Sensing is \textit{Compressive Sampling
% Matching Pursuit} algorithm or commonly known as \cosamp. 
In this discussion, one of the popular reconstruction algorithm that is \cosamp will be discussed, 
which belongs to the class of Greedy Algorithms.
The number of measurements needed for proper signal reconstruction is 
$m=O(k log(\frac{n}{k}))$ \cite{Needell-CoSaMP}. 
%no of measurements needed
\subsubsection{Notation}
Before giving the mathematical background of \cosamp, certain notations have to be introduced \cite{Needell-CoSaMP}.
For $x \in \mathbb{R}^n$ and $k$ a positive integer, $x_k$ implies retaining only the $k$ largest
values of \x and setting the rest equal to zero. Given the set $T \subseteq {1,2,...n}$, $\phi_T$ 
refers to the column submatrix of $\phi$ where columns are chosen as per the values in set $T$.
For the same set $T$, $x|_T$ is the restriction of \x to the set $T$, i.e. 
\begin{equation}
   x|_T = 
   \begin{cases}
    x_i, & \text{if} \ i \in T\\
    0, & otherwise.
   \end{cases}
  \end{equation}
  And pseudo-inverse of a matrix $A$ is defined as $A^\dagger= (A^\ast A)^{-1} A^\ast$.
  
% \begin{enumerate}
%   \item For $x \in \mathbb{R}^n$ and $k$ a positive integer, $x_k$ implies retaining only the $k$ largest
% values of \x and setting the rest equal to zero.
%   
%   \item For the set $T \subseteq {1,2,...n}$, $\phi_T$ refers to the column submatrix of $\phi$ where columns
%   are chosen as per the values in set $T$.
%   
%   \item Pseudo-inverse of a matrix $A$ is defined as, 
%   \begin{equation}
%    A^\dagger= (A^\ast A)^{-1} A^\ast
%   \end{equation}
% 
%   \item For $T \subseteq {1,2,...n}$, $x|_T$ is the restriction of \x to the set $T$, i.e. 
%   \begin{equation}
%    x|_T = 
%    \begin{cases}
%     x_i, & \text{if} \ i \in T\\
%     0, & otherwise.
%    \end{cases}
%   \end{equation}
% 
% \end{enumerate}

\subsubsection{Algorithm Description}
\label{Sec:cosamp_algo}
% \cosamp algorithm works by forming the signal \textit{proxy} and then use it to find the support set
% which is then used to estimate the signal \x, and then \textit{residual} is calculated, which
% is the part of the signal not estimated by the current estimation, and then this \textit{residual}
% is used to calculate the signal \textit{proxy} in the next iteration.
The \cosamp algorithm steps are 1)Forming the signal proxy and then pruning it to its \textit{k} largest
components; 2)Support merger; 3) Estimation using least squares; 4)Pruning the estimate to retain only the largest
\textit{k} entries; 5)Updating the residue. Here pruning refers to restricting \x to $x_k$ and residual
refers to the part of the signal which has not been estimated yet.\\
The pseudo-algorithm is described in Algorithm~\ref{CoSaMP} \cite{Needell-CoSaMP}.

  \begin{algorithm}
   \caption{CoSaMP algorithm}
   \label{CoSaMP}
   \begin{algorithmic}
    \Require{$\phi \in \mathbb{R}^{n \times m}$( measurement matrix ), 
    $y \in \mathbb{R}^m$( measurements ),\\ $k$( sparsity number )}\\
    \Ensure{$x \in \mathbb{R}^n$(recovered signal)}\\
    \State $x^0 \gets 0$
    \Comment{Initialisation}
    \State $y^r \gets y$
    \State $i \gets 0$\\
    \Repeat
    \State $i \gets i+1$
    \State $x^{temp} \gets \phi^\ast y^r$
    \Comment{Forming signal proxy}
    \State $\Omega \gets supp(x_{temp \ 2k})$
    \Comment{Restricting the proxy to its 2k largest component}
    \State $T \gets \Omega \cup supp(x^{i-1})$
    \Comment{Support Merger}
    \State $b|_T \gets \phi_T^\dagger y$
    \Comment{Estimating signal using least squares on the merged support set}
    \State $x^i \gets b_k$
    \State $y^r \gets y - \phi x^k$
    \Comment{Calculating residual}
    \Until $x^{i} - x^{i-1} \leq \epsilon$ 
    \Comment{For some small $\epsilon$}\\
    \Return $x = \phi^\dagger y$
    \Comment{Returning the best estimate when the algorithm converges}
   \end{algorithmic}
\end{algorithm}

\subsection{Dynamic Group Sparsity}
It might happen in certain cases that the $k$ nonzero coefficients of \x are not randomly distributed but clustered
into some groups, but this grouping keeps on changing and thus is called \textit{Dynamic Group Sparsity} or DGS.
\begin{defn}
 A vector $x \in \mathbb{R}^n$ is defined as \textit{dynamic group sparse} where \x is either \ksparse or can be well approximated
 by its \ksparse approximation and the \textit{k} non-zero coefficients of \x are grouped into $q \in \{1,2,...k\}$ groups.
\end{defn}
In DGS, we don't need to search over all $\binom{n}{k}$ subspaces, so the number of measurements required decreases to 
$m=O(k+q log(\frac{n}{q}))$ \cite{Huang-DGS} \cite{Li-CS_in_IoT}.
This is quite an improvement compared to \cosamp.

\par The DGS algorithm is similar to \cosamp algorithm  described in Section~\ref{Sec:cosamp_algo},
except steps 1 and 4. In DGS, pruning is done by using
DGS approximation pruning which is described in Algorithm~\ref{DGS_approx} \cite{Huang-DGS}. For DGS approximation
pruning, the neighbours of the signal as well as the weights assigned to those neighbours need to be specified.
  \begin{algorithm}
   \caption{DGS approximation algorithm}
   \label{DGS_approx}
   \begin{algorithmic}
    \Require{$x \in \mathbb{R}^n$ (vector to be pruned), $k$ (sparsity number), 
    $\tau$ (no. of neighbours of \x), $N_x \in \mathbb{R}^{n \times \tau}$ (values of $x$'s neighbours),
    $w \in \mathbb{R}^{n \times \tau}$ (weights assigned to neighbours).}\\
    \Ensure{$supp(x,k)$ (Support set of pruned signal)}\\
    \For{$i=1,..,n$}
    \State $z(i) = x^2(i) + \sum^{\tau}_{t=1} w^2(i,t) N_{x}^{2}(i,t)$\\
    \Comment{Calculating total contribution of a value and it's neighbours}
    \EndFor \\
    \State $\Omega \in \mathbb{R}^n \gets z_k$
    \Comment{Pruning $z$ to it's $k$ largest components}\\
    \Return $supp(x,k) \gets \Omega$
   \end{algorithmic}
\end{algorithm}
\section{Verification of Recovery Algorithms through Simulation}
  \subsection{Implementation}
  Both the \cosamp and DGS algorithm have been implemented for verification purpose and to understand how 
  number of measurements affect the convergence of algorithm and the amount of error.
  
  \par For test purpose, 1D measurement set having values $\pm 1$ was generated randomly with $n=512$, $k=64$ and $q=4$.
  Measurement matrix $\phi$ was generated choosing iid values from Gaussian distribution and then it was row normalized
  to have unit magnitude, $\tau$ was taken as $2$, the neighbours of the signal generated were taken as $\tau$ neighbouring
  values i.e. neighbours of $x(i)$ were $x(i-1)$ and $x(i+1)$ 
  and the weights $w$ was set as $0.5$ for all neighbours \cite{Huang-DGS}.
  \par First \cosamp was tried on it and then DGS. Results are decribed in the next section.
  \subsection{Discussion}
  \cosamp needs $m=O(klog(\frac{n}{k}))$ for successful signal recovery, $m=(factor+1)k$ was used where
  $factor=log(\frac{n}{k})$ and the algorithm converged in about $6$ iterations, as per the \textit{halting critereon}
  described in Algorithm~\ref{CoSaMP}. 
  \par Whereas for DGS, the number of measurements needed for convergence were $m=factor \ k$ and
  it took about $12$ iterations to converge. Error is of the same order as in \cosamp.
  %The generated data \x versus the estimated $x_{hat}$ for \cosamp and DGS is 
  The generated dataset \x is shown in Fig.1
  and the error with \cosamp and DGS is shown in Fig.2 and 
  Fig.3 respectively.\\
  \begin{figure}[H]
  \label{fig:dataset}
	\begin{center}
	\includegraphics[width=1.1\textwidth, height=3.5in]{data.eps}
	\caption{Generated data set \x} 
	\end{center}
	
\end{figure}
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1.1\textwidth, height=3.5in]{error_cosamp.eps}
	\caption{Error between \x and $x_{hat}$ with CoSaMP} 
	\end{center}
	\label{fig:error_cosamp}
\end{figure}
\begin{figure}[H]
	\begin{center}
	\includegraphics[width=1.1\textwidth, height=3.5in]{error_DGS.eps}
	\caption{Error between \x and $x_{hat}$ with DGS}
	\end{center}
	\label{fig:error_dgs}
\end{figure}
% \begin{figure}
%   \begin{subfigure}[H]{0.5\textwidth}
%     \includegraphics[width=\textwidth]{error_cosamp.eps}
%     \caption{Picture 1}
%     \label{fig:1}
%   \end{subfigure}
%   %
%   \begin{subfigure}[H]{0.5\textwidth}
%     \includegraphics[width=\textwidth]{error_DGS.eps}
%     \caption{Picture 2}
%     \label{fig:2}
%   \end{subfigure}
% \end{figure}
% \begin{tabular}{cc}
%   \num\putindeepbox[7pt]{\includegraphics[width=0.5\textwidth, height=2.5in]{data_cosamp.eps}}
%     & \num\putindeepbox[7pt]{\includegraphics[width=0.5\textwidth, height=2.5in]{data_DGS.eps}}
%   \end{tabular}
%   \begin{tabular}{cc}
%   \num\putindeepbox[7pt]{\includegraphics[width=0.5\textwidth, height=2.5in]{error_cosamp.eps}}
%     & \num\putindeepbox[7pt]{\includegraphics[width=0.5\textwidth, height=2.5in]{error_DGS.eps}} \\
%     %\caption{bcd}
%     \label{fig:all_results}
% \end{tabular}

\par If \cosamp is used with $m < (factor+1) k$, then the algorithm doesn't converge.

\section{Conclusion}
In the coming years, IoT is going to dominate all spheres of our life and so amount of data generated
will increase even further and with that there will be a dire need for algorithms which require 
fewer and fewer measurements and still provide signal reconstruction with small error.

\par \cosamp has been around for quite a long time and works equally well for all kind of data
sets irrespective of clustering or not. But for data with group clustering, DGS gives similar
performance as \cosamp but with fewer number of measurements.

\par DGS can be further improved to use lesser measurements and still provide proper reconstruction
by using dynamic weights $w$, which can be obtained by solving a least squares problem \cite{Li-CS_in_IoT}.
\par Overall, more algorithms which utilize the inherent properties of the data, 
like group clustering in this case, need to be developed.

\bibliographystyle{unsrt}%to make references appear in order of reference
\bibliography{seminar_report}


\end{document}
